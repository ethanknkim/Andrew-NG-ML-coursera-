2019-07-09
ML: Study that gives computer the ability to learn w.o. being explicitly programmed.
"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."

Example: playing checkers.

E = the experience of playing many games of checkers

T = the task of playing checkers.

P = the probability that the program will win the next game.

In general, any machine learning problem can be assigned to one of two broad classifications:

Supervised learning and Unsupervised learning.

Regression: Predict Continuous valued output

Classification: Discreted valued ouput

Training Set -> Learning Algorithm -> Hypothesis
x -> Hypothesis -> Estimated value
h = theta_0 + theta_1x

2019-07-10
Cost function = Squared Error Function: function of parameter theta_1(slope of hypothesis function)
Minimize 1/2m * sum of (h(x) - y)^2: Accuracy of hypothesis
h(x) = theta_0 + theta_1x

Gradient Descent(More Common Method)
Outline
start with theta_0,theta_1
keep change theta_0,theta_1 to minimize J(theta_0,theta_1) = 1/2m * sum of (h(x) - y)^2 to get to the minimum we want
Repeat Until Convergence {theta_j := theta_j - alpha * d/dtheta_j J(theta_0,theta_1)} (for j=0 and j=1)
alpha: learning rate
You simultaneously update theta_0 and theta_1
CORRECT WAY: compute temp0, temp1 and then assign temp0, temp1 to theta_0 and theta_1

2019-07-11
d/dtheta_j J(theta_0,theta_1) = d/dtheta_j 1/2m sum of (h(x)-y)^2
=d/dtheta_j*1/2m sum of (theta_0 + theta_1x - y)^2

Gradient Descent Algorithm
repeat until convergence {
theta_0 := theta_0 - alpha * 1/m sum of h(x)-y
theta_1 := theta_1 - alpha * 1/m sum of (h(x)-y)x
}

Batch Gradient Descent
"Batch": Each step of Gradient descent uses all the training examples

Matrix: Rectengular array of numbers
Dimension: # of rows x # of columns m-by-n matrix
A_ij ith row jth column
Scalar multiplication and addition: ez... learned in Univ.
Prediction = data matrix * parameters

2019-07-12
Multivariate Linear Regression
x_1 x_2 x_3.....(input variables)
y = (prediction variable)

n = number of features
x^(i) = ith training example
x^(i)_j = value of feature j in ith training example
X^(2) = 2nd training example
= [1416
3
2
40]
4-d vector it is.

Hypothesis :
h(x) = theta_0 + theta_1x_1 + theta_2x_2 +theta_3x_3 + theta_4x_4

For convenience, define x_0 = 1

Feature vector x = n+1 dimensional =[x0 x1 x2 ... xn]'
theta = n+1 dimensional = [theta_0 theta_1 ... theta_n]'

h(x) = theta' * x

New Gradient Descent Algorithm (n >=1)
Repeat {
theta_j := theta_j - alpha * 1/m * sum of (h(x^(i)) - y^(i))*x^(i)_j
} (simultaneously update thtea_j for j= 0, 1, 2, ...., n)

Feature Scaling
Idea: Make sure features are on a similar scale.

Get every feature into approx. -1=<x_i =<1 range.

mean normalization
replace x_i with x_i -  mu_i to make features have approx. 0 mean.
x1 <- x1-mu1/ s1
mu1 = average value of x1 in training set
s1 = range(max-min) or stdv.

Making Sure gradient descent is working correctly.
J(theta) should decrease after every iteration.

Example automatic convergence test: declare convergence if J(theta) decreases by less than 10^-3 in one iteration

Features and Polynomial Regression
Polynomial Regression

Normal Equation
Method to solve theta analytically
Intuition: you set derivative of J(theta) and make derivative 0
theta = (X'X)^-1X'y

Normal Equation noninvertibility
theta = (X'X)^-1*X'y
What if X'X is singular?
->Redundant features(linearly dependent)
->Too many features(e.g. m =< n)

2019-07-14
Classification
value y we want to predict is discret(0,1)
ex) email:spam/not spam, tumor: malignant/benign
y ={0,1}
0: "Negative Class"(benign tumor)
1: "Positive Class"(malignant tumor)
Hypothesis: = theta'*x
Threshold classifier output h(x) at 0.5:
h(x) >= 0.5, predict y=1
h(x) < 0.5, preidct y=0
Logistic Regression
: Want 0<=h(x)<=1
Hypothesis Representation
h(x) = g(theta'*x) =  1/(1+e^-(theta'*x))
g(z) = 1/(1+e^-z): logistic function(Sigmoid function)
Interpretation: h(x) = estimated probability that y=1 on input x
h(x) = p(y=1\x;theta) "probability that y=1, given x, parameterized by theta"

Decision Boundary
g(z) >= 0.5 when z >=0
therefore, h(x) >= 0.5 when g(theta'*x) >=0

ex: h(x) = g(theta0 + theta1*x1 + theta2*x2 )

Non-linear decision Boundary
2019-07-15

Cost function of Logistic Regression

Training set:{(x(1),y(1)), (x(2),y(2)), ...(x(m),y(m))}
m examples x = [x0 x1 x2 .... xn]' x0=1, y={0,1} => n+1 dimension

How to choose parameters theta?
Cost(h(x), y) = 1/2(h(x)-y)^2

Cost(h(x),y) = -log(h(x)) for y=1, -log(1-h(x)) for y =0
Cost(hθ(x),y)→∞ if y=0andhθ(x)→1
Cost(hθ(x),y)→∞ if y=1andhθ(x)→0
If our correct answer 'y' is 0, then the cost function will be 0 if our hypothesis function also outputs 0. If our hypothesis approaches 1, then the cost function will approach infinity.

If our correct answer 'y' is 1, then the cost function will be 0 if our hypothesis function outputs 1. If our hypothesis approaches 0, then the cost function will approach infinity.

Note that writing the cost function in this way guarantees that J(θ) is convex for logistic regression.

Simplified Cost Function and Gradient Descent
Cost(h(x),y) = -y*log(h(x)) - (1-y)*log(1-h(x))
J(theta) = -1/m*[sum of y*logh(x) + (1-y)*log(1-h(x))]

To fit parameters theta:
min J(theta)
To make a prediction given new x:
Output h(x) = 1/(1 + e^(-theta'*x))

Gradient Descent
Repeat {
	theta_j = theta_j -alpha * d/dtheta_j J(theta) = theta_j - alpha * sum of (h(x)-y)*x
}h(x) = 1/(1+e^(-theta'*x))
Algorithm looks identical to linear regression
You need to monitor convergence

J(θ)=1/m*(−y'*log(h)−(1−y)'*log(1−h))
A vectorized implementation is:
theta = theta - alpha/m*X'*(g(X*theta) - y)

Advanced Optimization
Cost function J(theta), Want min J(theta)
Given theta, we have code that can compute
J(theta) and d/dtheta_j J(theta)

Optimization Algorithms:
- Gradient Descent
- Conjugate gradient
- BFGS
- L-BFGS

No need to manually pick alpha
Often faster than gradient descent

Multiclass Classification: One-vs-all method
ex: Email Foldering/tagging : Work, Friends, Family, Hobby y = { 1, 2, 3, 4}
You make multiple h(x)
h(i)(x) = P(y=i|x;theta) (i=1,2,3..)

Train a logistic regression classifier h(x) for each class i to predict the probability that y = i
prediction = max(h(x))

Overfitting
underfit = high bias
overfit = high variance
: With too many features, the learned hypothesis may fit the training set very well but fail to generalize to new examples.

Addressing overfitting:
1. Reduce number of features
- manually select which features to keep
- model selection algorithm
2. Regularization
- keep all the features, but reduce magnitude/values of parameters theta_j
-works well when we have a lot of features

Cost function(Regularization)
Intuition: make high order theta really small
Small values for parameters
-simpler hypothesis (lower order)
-less prone to overfitting
add extra regularization term at the end
J(theta) = 1/2m * sum of (h(x) - y)^2 + lambda * sum of theta_j^2
lambda = regularization parameter : It determines how much the costs of our theta parameters are inflated.

2019-07-16
Regularized Linear Regression

theta_j = theta_j*(1-alpha * lambda/m)-alpha/m* sum of (h(x)-y)*xj, (1-alpha * lambda/m)<1

Normal equation

X=[x1'; x2'; .....xm'], y = [y1, y2, ...ym]'

theta = (X'X+lambda[0 0 0 0.. 0)^-1 *X'y
		   0 1 0 0  0
		   0 0 1 0 . 0]

Non-invertibility
Suppose m(# of examples) <= n(#of features),

Regularized Logistic Regression
J(theta)  = -[1/m * sum of y*loglambda/2m * sum of theta_j^2 for j=1....n

Neural Networks
Model Representations
Dendrites: input features
output: result of hypothesis function

Forward propagation: vectorized implementation
a1(2) = g(theta10(1)*x0 +theta11(1)x1 + theta12(1)x2 + theta13(1)x3

2019-07-17
Non-linear classification example: XOR/XNOR

y = x1 XOR x2
x1 XNOR x2 = NOT(x1 XOR x2)

-10 +20x1 + 20x2
x1 x2 g
0 0 -10
1 0 10
0 1 10
1 1 30

(not x1) and (not x2)
x1 not x1 x2 not x2
0 1 0 1
1 0 1 0
x1 x2 not x1 not x2 g
0 0 1 1 1
1 0 0 1 0
0 1 1 0 0
1 1 0 0 0

Multiclass classification
To classify data into multiple classes, we let our hypothesis function return a vector of values.

Neural Network (Classification)
L = total no. of layers
s_l = no. of units in layer l
Binary / Multi-class Classification
1 output unit / K output unit
